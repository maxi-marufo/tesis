\chapter{Métodos de Evaluación}\label{evaluacion}

La evaluación de métodos de cuantificación es más compleja que en otros
problemas. En aprendizaje supervisado, típicamente se mide el rendimiento
estimando la probabilidad de predecir correctamente ejemplos individuales no
observados. Sin embargo, en cuantificación, el rendimiento se evalúa para
conjuntos de datos. Esto implica que necesitamos una colección de muestras para
evaluar el rendimiento de un método. Dado un método $\overline{h}$, una función
de pérdida $L(\cdot, \cdot)$, y un conjunto de muestras de evaluación ${T_1,
\dots, T_s}$, el rendimiento de $\overline{h}$ es:

\begin{equation}
    Rendimiento(\overline{h}, L, {T_1, \dots , T_s}) = \frac{1}{s}
    \sum \limits_{j=1}^{s}L(\overline{h}, T_j)
    \label{ecuacion_rendimiento}
\end{equation}

Calcular la pérdida de un método de cuantificación sobre una muestra de prueba
($L(\overline{h}, T_j)$) no equivale a promediar o sumar pérdidas obtenidas a
nivel individual (es decir, pérdidas para clasificación o regresión), debido a
la interdependencia de estas últimas a nivel muestra. Por ejemplo, para el caso
binario, si en una muestra de evaluación hay una mayor incidencia de falsos
positivos que de falsos negativos, la presencia de un falso negativo adicional
puede en realidad mejorar el error de cuantificación general, gracias al efecto
de compensación mutua entre $FP$ y $FN$ mencionado
en~\ref{problema:clasificar_y_contar}.

Además, el problema de evaluación en cuantificación se relaciona con el cambio
en la distribución de datos entre la fase de entrenamiento y la de
implementación del método. Se requiere una colección de muestras de prueba
variada y que represente diversas distribuciones para evaluar correctamente el
rendimiento del método y evitar sesgos. Por esta razón, la mayoría de los
experimentos reportados en la literatura emplean conjuntos de datos tomados de
otros problemas y se crean conjuntos de prueba con cambios en las distribuciones
creados artificialmente. Este enfoque tiene la ventaja de que la cantidad del
{\it dataset shift\/} se puede controlar para estudiar el rendimiento de los
métodos en diferentes situaciones.

Las funciones de pérdida $L(\cdot, \cdot)$ serán elegidas de acuerdo al tipo de
problema y al objetivo particular de la aplicación. Como ya se mencionó, el
rendimiento de $\overline{h}$ será el promedio del resultado de la función de
pérdida por cada muestra de evaluación, de acuerdo a la
ecuación~\ref{ecuacion_rendimiento}. Se han propuesto en la literatura distintas
medidas de evaluación para problemas de {\it Single-Label Quantification (SLQ)}.
Estas también se pueden usar para {\it Binary Quantification (BQ)}, ya que es un
caso espacial del anterior, y para {\it Multi-Label Quantification}, ya que se
pueden usar para cada $y \in C$. Esencialmente todas las medidas de evaluación
que se han propuesto son divergencias, es decir, medidas de cómo una
distribución difiere de otra. No se desarrollarán en esta tesis medidas para
{\it Ordinal Quantification\/} ni para {\it Regression Quantification}, ya que
no son útiles para nuestro objeto de estudio.

\section{Propiedades}\label{evaluacion:propiedades}

~\citet{sebastiani2020evaluation} define una serie de propiedades interesantes
para medidas de evaluación en {\it SLQ}. Un importante resultado de este
artículo es que ninguna medida de evaluación existente para {\it SLQ\/}
satisface todas las propiedades identificadas como deseables; aún así, se ha
demostrado que algunas medidas de evaluación son “menos inadecuadas” que otras.
Aquí mencionamos brevemente las cuatro propiedades principales que habría que
considerar en cada medida $M$ a emplear (el resto son propiedades que suelen ser
satisfechas por todas las medidas).

\begin{itemize}
    \item {\bf Máximo (MAX)}: si $\exists \beta >0, \beta \in \mathbb{R}$ tal
    que por cada $c \in C$ y por cada $p$, (i) existe $\hat p$ tal que $M(p,
    \hat p) = \beta$, y (ii) para ninguna $\hat p$ se cumple que $M(p, \hat p) >
    \beta$. Si se cumple {\bf MAX}, la imagen de $M$ es independiente del
    problema, y esto permite juzgar si un valor dado significa un error de
    cuantificación alto o bajo. Si $M$ no cumple cumple {\bf MAX}, cada muestra
    de evaluación tendrá un peso distinto en el resultado final.
    \item {\bf Imparcial (IMP)}: si $M$ penaliza igualmente la subestimación de
    $p$ por una cantidad $a$ (es decir, con $\hat p = p - a$) o su
    sobreestimación por la misma cantidad $a$ (es decir, con $\hat p = p + a$).
    Si se cumple {\bf IMP}, la subestimación y la sobreestimación se consideran
    igualmente indeseables. Esto es generalmente lo deseable, a menos que exista
    una razón específica para no hacerlo.
    \item {\bf Relativo (REL)}: si $M$ penaliza más gravemente un error de
    magnitud absoluta $a$ (es decir, cuando $\hat p = p \pm a)$ si $p$ es menor.
    Por ejemplo, predecir $\hat p = 0.0101$ cuando $p = 0.0001$ es un error
    mucho más serio que predecir $\hat p = 0.1100$ cuando $p = 0.1000$.
    \item {\bf Absoluto (ABS)}: si $M$ penaliza un error de magnitud
    independientemente del valor de $p$. Mientras algunas aplicaciones requieren
    {\bf REL}, otras requieren {\bf ABS}. Si bien {\bf REL} y {\bf ABS} son
    mutuamente excluyentes, ninguna cubre el caso cuando $M$ considera un error
    de magnitud absoluta $a$ menos grave cuando $p$ es menor (como en el caso de
    la {\it distancia coseno\/}).
\end{itemize}

\section{Medidas de Evaluación}\label{evaluacion:medidas}

El rendimiento se calcula haciendo un promedio entre un conjunto de muestras de
evaluación~\ref{ecuacion_rendimiento}, por lo que la definición matemática de
cada medida de evaluación (o función de pérdida) se reduce a obtener un
''puntaje'' para una sola muestra. Como ya mencionamos, todas las medidas de
evaluación usadas en cuantificación son divergencias. Una divergencia $D$ es una
medida de cómo una distribución predicha $\hat{p}$ diverge (es decir, difiere)
de la distribución real $p$, y se define de tal manera que (1) $D(p, \hat{p}) =
0$ si y solo si $p = \hat{p}$, y (2) $D(p, \hat{p}) > 0$ para todo $\hat{p} \neq
p$. Para cuantificación binaria, el problema se reduce a la predicción de la
proporción de la clase positiva, $p$. Así, para un modelo $\overline{h}$ y un
conjunto de prueba dado $T$, solo necesitamos comparar la prevalencia predicha,
$\hat{p}$, con la real, $p$.

\subsubsection{Error {\normalfont(solo para Cuantificación
Binaria)}}\label{evaluacion:error}

El error técnicamente no es una medida de evaluación para la cuantificación, ya
que no se aplica a toda una distribución $p$ sino solo a una clase específica $c
\in C$, y se define como:
\begin{equation}
    {\text{e}(c)} = \hat p(c) - p(c)\label{evaluacion:eq_e}
\end{equation}

Incluso usado en cuantificación binaria, se debe especificar a cuál de las
clases hace referencia (en este caso, como bien dijimos, suele hacer referencia
a la clase positiva). Si se usa como una medida de evaluación para la
cuantificación, un problema con el error es que promediar los valores para
diferentes clases produce resultados poco intuitivos, ya que el error positivo
de una clase y el error negativo de otra clase se anulan entre sí. El mismo
problema ocurre cuando se trata de la misma clase pero se promedia entre
diferentes muestras. Como resultado, esta medida se puede utilizar como mucho
para determinar si un método tiene una tendencia a subestimar o sobrestimar la
prevalencia de una clase específica (esto es, promediar el error para una clase
particular entre distintas muestras, y de esta manera estimar el sesgo o {\it
bias}) en {\it BQ}, y no como una medida de evaluación general para usar.

Las siguientes medidas de evaluación se pueden emplear en cuantificación
multiclase ya que, suman siempre valores positivos para todas las clases.

\subsubsection{Error Absoluto}\label{evaluacion:ae}

El error absoluto o {\it absolute error\/} es una de las medidas más empleadas
ya que, al ser simplemente la diferencia entre ambas magnitudes, es simple y
fácilmente interpretable.
\begin{equation}
    {\text{AE}(p, \hat p)} = \frac{1}{\#C}\sum \limits_{j=1}^{\#C}{|\hat p(c=c_j) - p(c=c_j)|}\label{evaluacion:eq_ae}
\end{equation}

Como en este caso las diferencias positivas y negativas son igualmente
indeseables, promediar el AE entre varias clases, o varias muestras, no es
problemático. Como se muestra en~\cite{sebastiani2020evaluation}, AE cumple {\bf
IMP} y {\bf ABS} pero no cumple {\bf MAX} (ni tampoco {\bf REL}). Su rango va de
0 (mejor) a:
\begin{equation}
    z_{\text{AE}} = \frac{2(1-\displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j))}{\#C}\label{evaluacion:eq_zae}
\end{equation}
(peor), por lo que su rango depende de la distribución de $p$ y de $\#C$.

\subsubsection{Error Absoluto Normalizado}\label{evaluacion:nae}

El error absoluto normalizado {\it normalised absolute error}, definido como:
\begin{equation}
    {\text{NAE}(p, \hat p)} = \frac{\text{AE}(p, \hat p)}{z_{\text{AE}}} = \frac{\sum \limits_{j=1}^{\#C}{|\hat p(c=c_j) - p(c=c_j)|}}{2(1-\displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j))}\label{evaluacion:eq_nae}
\end{equation}
es una versión de AE que oscila entre 0 (mejor) y 1 (peor), por lo que cumple
{\bf MAX}. A pesar de su nombre, NAE no disfruta de {\bf ABS} (ni tampoco {\bf
REL}).

\subsubsection{Error Cuadrático}\label{evaluacion:se}

El error cuadrático o {\it squared error}, definido como:
\begin{equation}
    {\text{SE}(p, \hat p)} = \frac{1}{\#C}\sum \limits_{j=1}^{\#C}{{(\hat p(c=c_j) - p(c=c_j))}^2}\label{evaluacion:eq_se}
\end{equation}
comparte los mismos pros y contras de AE, pero penalizando más cuanto mayor es
la diferencia entre el valor real y el predicho, por lo que se usa cuando se
quiere castigar los valores atípicos u {\it outliers}.

\subsubsection{Error Absoluto Relativo}\label{evaluacion:rae}

El error absoluto relativo o {\it relative absolute error\/} es una adaptación
del AE que impone {\bf REL} al hacer que AE sea relativo a $p$.
\begin{equation}
    {\text{RAE}(p, \hat p)} = \frac{1}{\#C}\sum \limits_{j=1}^{\#C}{\frac{|\hat p(c=c_j) - p(c=c_j)|}{p(c=c_j)}}\label{evaluacion:eq_rae}
\end{equation}

RAE cumple {\bf IMP} y {\bf REL} pero no cumple {\bf MAX} (ni {\bf ABS}, a pesar
de su nombre). Su rango va de 0 (mejor) a:
\begin{equation}
    z_{\text{RAE}} = \frac{\#C - 1 + \frac {1 - \displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j)}{\displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j)}}{\#C}\label{evaluacion:eq_zrae}
\end{equation}
(peor), por lo que su rango depende de la distribución de $p$ y de $\#C$.

\subsubsection{Error Absoluto Relativo Normalizado}\label{evaluacion:nrae}

El error absoluto relativo normalizado {\it normalised relative absolute error},
definido como:
\begin{equation}
    {\text{NRAE}(p, \hat p)} = \frac{\text{RAE}(p, \hat p)}{z_{\text{RAE}}} = \frac{\sum \limits_{j=1}^{\#C}{\frac{|\hat p(c=c_j) - p(c=c_j)|}{p(c=c_j)}}}{\#C - 1 + \frac {1 - \displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j)}{\displaystyle \min_{j\in\{1,\dots,\#C\}}p(c=c_j)}}\label{evaluacion:eq_nrae}
\end{equation}
es una versión de RAE que oscila entre 0 (mejor) y 1 (peor), por lo que cumple
{\bf MAX}. A pesar de su nombre, NRAE no disfruta de {\bf REL} (ni tampoco {\bf
ABS}).

Tanto RAE como NRAE no están definidas cuando sus denominadores sean nulos. Para
resolver este problema, se puede suavizar tanto $p(c=c_j)$ como $\hat p(c=c_j)$
mediante suavizado aditivo:
\begin{equation}
    \underline p(c=c_j) = \frac{\epsilon + p(c=c_j)}{\epsilon  \#C + \sum \limits_{j=1}^{\#C}{p(c=c_j)}}\label{evaluacion:eq_suav_rae_nrae}
\end{equation}
donde $\underline p(c=c_j)$ es la versión suavizada de $p(c=c_j)$ y el
denominador es solo un un factor de normalización (lo mismo para $\underline
{\hat p}(c=c_j)$).

\subsubsection{Divergencia de Kullback-Leibler}\label{evaluacion:dkl}

Para distribuciones de probabilidad discretas $P$ y $Q$ definidas en el mismo
espacio muestral ${\mathcal {X}}$ su divergencia KL se define como:
\begin{equation}
    {\text{DKL}}(P\parallel Q)=\sum \limits_{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right)\label{evaluacion:eq_dkl}
\end{equation}

En cuantificación, se quiere comparar la prevalencia real $p$ y la prevalencia
predicha $\hat{p}$, y el espacio muestral corresponde a las posibles clases, con
lo cuál será:
\begin{equation}
    {\text{DKL}}(p\parallel \hat{p}) = \sum \limits_{j=1}^{\#C}p(c=c_j)\log \left({\frac {p(c=c_j)}{\hat p(c=c_j)}}\right)\label{evaluacion:eq_dkl2}
\end{equation}
que va de {0} (mejor) a {+$\infty$} (peor) -por lo tanto, no cumple con {\bf
MAX}-. Esta medida no es simétrica, es menos interpretable que otras medidas de
rendimiento, y no está definido cuando $\hat{p}$ es 0 o 1.

\subsubsection{Divergencia de Kullback-Leibler
Normalizada}\label{evaluacion:ndkl}

Para suplir los problemas de DKL, se puede utilizar la función logística,
quedando:
\begin{equation}
    {\text{NDKL}}(p\parallel \hat{p}) = 2 \cdot \frac{e^{{\text{DKL}}(p\parallel \hat{p})}}{1+e^{{\text{DKL}}(p\parallel \hat{p})}}-1\label{evaluacion:eq_ndkl}
\end{equation}
que va de {0} (mejor) a {1} (peor) -por lo tanto, si cumple con {\bf MAX}-. Sin
embargo, como se muestra en~\cite{sebastiani2020evaluation}, ni DKL ni NDKL
cumplen con {\bf IMP}, {\bf REL} y {\bf ABS}, lo que hace que su uso como
medidas de evaluación para cuantificación sea cuestionable, además de ser
difíciles de interpretar.

\section{Elección de la Medida de Evaluación}\label{evaluacion:eleccion}

Es evidente que ninguna de las medidas propuestas hasta ahora es completamente
satisfactoria. DKL y NDKL son los menos satisfactorios y parecen fuera de
discusión. Respecto a los demás, el problema es que {\bf MAX} parece ser
incompatible con {\bf REL}/{\bf ABS}, y viceversa.

~\citet{sebastiani2020evaluation} sostiene que cumplir con {\bf REL} o {\bf ABS}
parece más importante que cumplir con {\bf MAX}, ya que reflejan las necesidades
de la aplicación; si no se satisfacen estas propiedades, se puede argumentar que
el error de cuantificación que se está midiendo está vagamente relacionado a lo
que el usuario realmente quiere. Si {\bf MAX} no está satisfecho, los resultados
obtenidos en muestras caracterizadas por diferentes distribuciones no serán
comparables. A pesar de esto, los resultados obtenidos por diferentes sistemas
en el mismo conjunto de muestras siguen siendo comparables.

Esto sugiere que AE, RAE y SE son las mejores medidas a elegir. Se debe preferir
AE cuando un error de estimación de una magnitud absoluta dada debe considerarse
más grave cuando la verdadera prevalencia de la clase afectada es menor. RAE
debe ser elegido cuando un error de estimación de una magnitud absoluta dada
tiene el mismo impacto independientemente de la verdadera prevalencia de la
clase afectada. Si se quiere penalizar mayormente errores atípicos, considerando
mucho más graves a los errores cuanto mayor es la diferencia entre el valor real
y el predicho, entonces SE es la medida más conveniente.

\section{Protocolos}\label{evaluacion:protocolos}

Mientras que en la clasificación, un conjunto de datos de tamaño $k$ proporciona
$k$ puntos de evaluación, para la cuantificación, el mismo conjunto solo
proporciona $1$ punto. Evaluar algoritmos de cuantificación es por lo tanto un
reto, debido a que la disponibilidad de datos etiquetados con fines de prueba es
más restringido. Hay principalmente dos protocolos experimentales que se han
tomado para tratar con este problema: el Protocolo de Prevalencia Natural ({\it
NPP\/}) y el Protocolo de Prevalencia Artificial ({\it APP\/}).

\begin{itemize}
    \item {\it NPP\/}: Consiste en, una vez entrenado un cuantificador, tomar un
    conjunto de prueba (no observado en el entrenamiento) lo suficientemente
    grande, dividirlo en un número de muestras de manera uniformemente
    aleatoria, y llevar a cabo la evaluación individualmente en cada muestra.
    \item {\it APP\/}: Consiste en, previo al entrenamiento, tomar un conjunto
    de datos, dividirlo en un conjunto de entrenamiento y en un conjunto de
    evaluación de manera aleatoria, y realizar experimentos repetidos en los que
    la prevalencia del conjunto de entrenamiento o la prevalencia del conjunto
    de prueba de una clase se varía artificialmente a través del submuestreo.
\end{itemize}

Ambos protocolos tienen diferentes pros y contras. Una ventaja de {\it APP\/} es
que permite crear muchas puntos de prueba de la misma muestra. Además, {\it
APP\/} permite simular distintos {\it Prior probability shift}, mientras que con
{\it NPP\/} se estaría evaluando sólo con las distribuciones originales de los
datos de entrenamiento y prueba. Sin embargo, una desventaja de {\it APP\/} es
que puede no saberse cuán realistas son estas diferentes situaciones en la
aplicación real, por lo que se podría estar destinando recursos a una evaluación
errónea o pobre. Una solución intermedia podría ser utilizar un protocolo que
utilice conocimientos previos sobre la distribución de prevalencias “probables”
que se podría esperar encontrar en el dominio específico en cuestión.

\section{Selección de Modelos}\label{evaluacion:seleccion}

El rendimiento de muchos algoritmos de aprendizaje automático es altamente
sensible a la configuración de sus hiperparámetros. Estos hiperparámetros, a
diferencia de los parámetros, no se aprenden durante el entrenamiento, debiendo
ser ajustados previamente para cada problema específico. Aunque muchos
algoritmos ofrecen valores predeterminados, la optimización de estos
hiperparámetros es esencial para maximizar el rendimiento en aplicaciones
concretas. Los métodos de cuantificación no son una excepción en este
sentido~\cite{esuli2023learning}.

El proceso de selección de modelos consiste en evaluar diferentes combinaciones
de hiperparámetros hasta quedarse con la combinación que obtenga la mejor
evaluación. Para garantizar una evaluación rigurosa, es recomendable utilizar
validación cruzada. En el caso de la cuantificación, la optimización de
hiperparámetros debería imitar el protocolo de evaluación al evaluar cada una de
las configuraciones candidatas. En otras palabras, dado que el objetivo de la
selección de modelos es encontrar la configuración de hiperparámetros que
funcione mejor de acuerdo con un protocolo experimental dado y una medida de
evaluación dada, resulta adecuado adoptar los mismos protocolos de evaluación
(\ref{evaluacion:protocolos}) y medidas de evaluación (\ref{evaluacion:medidas})
que se usan habitualmente en la evaluación de sistemas de
cuantificación~\cite{moreo2022tweet, hassan2021pitfalls}.

Algunos de los métodos de cuantificación que presentan hiperparámetros {\it
per-se\/} y que hemos mencionado en el Capítulo~\ref{estimacion} son el {\it
HDy\/} y {\it HDx\/} (con respecto a la cantidad de {\it bins}) y {\it ACC\/} y
{\it PACC\/} para el caso multiclase (con respecto al método para resolver el
sistema de ecuaciones lineales), entre otros. Adicionalmente, todos los métodos
agregativos (\ref{estimacion:agregativos}) heredan los hiperparámetros de los
clasificadores que emplean. En este sentido, Moreo y
Sebastiani~\cite{moreo2021re} afirman que el método {\it CC\/} y sus variantes,
con una adecuada optimización, pueden ofrecer un rendimiento competitivo, aunque
siguen siendo inferiores a los métodos de cuantificación más sofisticados,
además de requerir recursos y tiempo de cómputo para la búsqueda de
hiperparámetros.
